{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognize named entities on news data with CNN\n",
    "In this tutorial, you will use a convolutional neural network to solve Named Entity Recognition (NER) problem. NER is a common task in natural language processing systems. It serves for extraction such entities from the text as persons, organizations, locations, etc. In this task you will experiment to recognize named entities in different documents from s dataset.\n",
    "\n",
    "For example, we want to extract persons' and organizations' names from the text. Then for the input text:\n",
    "\n",
    "Yan Goodfellow works for Google Brain\n",
    "\n",
    "a NER model needs to provide the following sequence of tags:\n",
    "\n",
    "B-REF I-REF O\n",
    "\n",
    "Where B- and I- prefixes stand for the beginning and inside of the entity, while O stands for out of tag or no tag. Markup with the prefix scheme is called BIO markup. This markup is introduced for distinguishing of consequent entities with similar types.\n",
    "\n",
    "A solution of the task will be based on neural networks, particularly, on Convolutional Neural Networks.\n",
    "\n",
    "## Data\n",
    "The following cell will be in folder /data. \n",
    "\n",
    "We will work with a corpus, which contains twits with NE tags. Typical file with NER data contains lines with pairs of tokens (word/punctuation symbol) and tags, separated by a whitespace. In many cases additional information such as POS tags included between Different documents are separated by lines started with -DOCSTART- token. Different sentences are separated by an empty line. Example\n",
    "\n",
    "-DOCSTART- -X- -X- O\n",
    "\n",
    "По O\n",
    "защите O\n",
    "прав O\n",
    "граждан O\n",
    "( O\n",
    "определения O\n",
    "от B-REF\n",
    "21 I-REF\n",
    "декабря I-REF\n",
    "1998 I-REF\n",
    "года I-REF\n",
    "№ I-REF\n",
    "183-O I-REF\n",
    ". O\n",
    "\n",
    "А O\n",
    "также O\n",
    "\n",
    "We start with using the Conll2003DatasetReader class that provides functionality for reading the dataset. It returns a dictionary with fields train, test, and valid. At each field a list of samples is stored. Each sample is a tuple of tokens and tags. Both tokens and tags are lists. The following example depicts the structure that should be returned by read method:\n",
    "\n",
    "```python\n",
    "{'train': [(['с', 'определением', 'от','24','декабря','2003','года','№','156-O'], [ 'O', 'O', 'B-REF','I-REF','I-REF','I-REF'.'I-REF','I-REF','I-REF']), ....],\n",
    " 'valid': [...],\n",
    " 'test': [...]}\n",
    "```\n",
    "\n",
    "There are three separate parts of the dataset:\n",
    "\n",
    "train data for training the model;\n",
    "validation data for evaluation and hyperparameters tuning;\n",
    "test data for final evaluation of the model.\n",
    "Each of these parts is stored in a separate txt file.\n",
    "\n",
    "We will use Conll2003DatasetReader from the library to read the data from text files to the format described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.dataset_readers.conll2003_reader import Conll2003DatasetReader\n",
    "dataset = Conll2003DatasetReader().read('/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dictionaries\n",
    "To train a neural network, we will use two mappings:\n",
    "\n",
    "{token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
    "{tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
    "Token indices will be used to address the row in embeddings matrix. The mapping for tags will be used to create one-hot ground truth probability distribution vectors to compute the loss at the output of the network.\n",
    "\n",
    "The SimpleVocabulary implemented in the library will be used to perform those mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to build dictionaries for tokens and tags. Sometimes there are special tokens in vocabularies, for instance an unknown word token, which is used every time we encounter out of vocabulary word. In our case the only special token will be<UNK> for out of vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:34:18.944 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 48: No load path is set for SimpleVocabulary in 'infer' mode. Using save path instead\n",
      "2018-11-18 23:34:18.949 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 48: No load path is set for SimpleVocabulary in 'infer' mode. Using save path instead\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['<UNK>']\n",
    "\n",
    "token_vocab = SimpleVocabulary(special_tokens, save_path='model/token.dict')\n",
    "tag_vocab = SimpleVocabulary(save_path='model/tag.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fit the vocabularies on the train part of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_by_sentences = [tokens for tokens, tags in dataset['train']]\n",
    "all_tags_by_sentences = [tags for tokens, tags in dataset['train']]\n",
    "\n",
    "token_vocab.fit(all_tokens_by_sentences)\n",
    "tag_vocab.fit(all_tags_by_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Try to get the indices. Keep in mind that we are working with batches of the following structure:\n",
    "\n",
    "[['utt0_tok0', 'utt1_tok1', ...], ['utt1_tok0', 'utt1_tok1', ...], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[54, 0, 9, 3]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_vocab([['Конституции', 'Российскийской', 'Федерации', 'от']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0], [2, 1]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_vocab([['O', 'O', 'O'], ['B-REF', 'I-REF']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Iterator\n",
    "Neural Networks are usually trained with batches. It means that weight updates of the network are based on several sequences at every single time. The tricky part is that all sequences within a batch need to have the same length. So we will pad them with a special <UNK> token. Likewise tokens tags also must be padded It is also a good practice to provide RNN with sequence lengths, so it can skip computations for padding parts. We provide the batching function batches_generator readily available for you to save time.\n",
    "\n",
    "An important concept in the batch generation is shuffling. Shuffling is taking sample from the dataset at random order. It is important to train on the shuffled data because large number consequetive samples of the same class may result in pure quality of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.data_learning_iterator import DataLearningIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset iterator from the loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = DataLearningIterator(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((['К',\n",
       "   'тому',\n",
       "   'же',\n",
       "   'при',\n",
       "   'осуществлении',\n",
       "   'в',\n",
       "   'период',\n",
       "   'предварительного',\n",
       "   'расследования',\n",
       "   'судебного',\n",
       "   'контроля',\n",
       "   'за',\n",
       "   'законностью',\n",
       "   'и',\n",
       "   'обоснованностью',\n",
       "   'процессуальных',\n",
       "   'актов',\n",
       "   'органов',\n",
       "   'дознания',\n",
       "   ',',\n",
       "   'следователей',\n",
       "   'и',\n",
       "   'прокуроров',\n",
       "   'судом',\n",
       "   'не',\n",
       "   'должны',\n",
       "   'предрешаться',\n",
       "   'вопросы',\n",
       "   ',',\n",
       "   'которые',\n",
       "   'впоследствии',\n",
       "   'могут',\n",
       "   'стать',\n",
       "   'предметом',\n",
       "   'судебного',\n",
       "   'разбирательства',\n",
       "   'по',\n",
       "   'существу',\n",
       "   'уголовного',\n",
       "   'дела',\n",
       "   '(',\n",
       "   'Постановление',\n",
       "   'Конституционного',\n",
       "   'Суда',\n",
       "   'Российской',\n",
       "   'Федерации',\n",
       "   'от',\n",
       "   '23',\n",
       "   'марта',\n",
       "   '1999',\n",
       "   'года',\n",
       "   '№',\n",
       "   '5-П',\n",
       "   ';',\n",
       "   'определения',\n",
       "   'Конституционного',\n",
       "   'Суда',\n",
       "   'Российской',\n",
       "   'Федерации',\n",
       "   'от',\n",
       "   '27',\n",
       "   'мая',\n",
       "   '2010',\n",
       "   'года',\n",
       "   '№',\n",
       "   '633-О-О',\n",
       "   ',',\n",
       "   'от',\n",
       "   '14',\n",
       "   'июля',\n",
       "   '2011',\n",
       "   'года',\n",
       "   '№',\n",
       "   '1027-О-О',\n",
       "   ',',\n",
       "   'от',\n",
       "   '21',\n",
       "   'ноября',\n",
       "   '2013',\n",
       "   'года',\n",
       "   '№',\n",
       "   '1877-О',\n",
       "   ',',\n",
       "   'от',\n",
       "   '29',\n",
       "   'января',\n",
       "   '2015',\n",
       "   'года',\n",
       "   '№',\n",
       "   '30-О',\n",
       "   'и',\n",
       "   'др',\n",
       "   '.',\n",
       "   ')',\n",
       "   '.'],),\n",
       " (['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'O',\n",
       "   'B-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'O',\n",
       "   'B-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'O',\n",
       "   'B-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'I-REF',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(data_iterator.gen_batches(1, shuffle=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking\n",
    "The last thing about generating training data. We need to produce a binary mask which is one where tokens present and zero elsewhere. This mask will stop backpropagation through paddings. An instance of such mask:\n",
    "\n",
    "[[1, 1, 0, 0, 0],\n",
    " [1, 1, 1, 1, 1]]\n",
    " \n",
    "For the sentences in batch:\n",
    "\n",
    " [['The', 'roof'],\n",
    "  ['This', 'is', 'my', 'domain', '!']]\n",
    "\n",
    "The mask length must be equal to the maximum length of the sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\stron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     C:\\Users\\stron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     C:\\Users\\stron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.preprocessors.mask import Mask\n",
    "get_mask = Mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mask([['Try', 'to', 'get', 'the', 'mask'], ['Check', 'paddings']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a recurrent neural network\n",
    "This is the most important part of the assignment. Here we will specify the network architecture based on TensorFlow building blocks. It's fun and easy as a lego constructor! We will create an Convolutional Neural Network (CNN) network which will produce probability distribution over tags for each token in a sentence. To take into account both right and left contexts of the token, we will use CNN. Dense layer will be used on top to perform tag classification.\n",
    "An essential part of almost every network in NLP domain is embeddings of the words. We pass the text to the network as a series of tokens. Each token is represented by its index. For every token (index) we have a vector. In total the vectors form an embedding matrix. This matrix can be either pretrained using some common algorithm like Skip-Gram or CBOW or it can be initialized by random values and trained along with other parameters of the network. In this tutorial we will follow the second alternative.\n",
    "\n",
    "We need to build a function that takes the tensor of token indices with shape [batch_size, num_tokens] and for each index in this matrix it retrieves a vector from the embedding matrix, corresponding to that index. That results in a new tensor with sahpe [batch_size, num_tokens, emb_dim]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def get_embeddings(indices, vocabulary_size, emb_dim):\n",
    "    # Initialize the random gaussian matrix with dimensions [vocabulary_size, embedding_dimension]\n",
    "    # The **VARIANCE** of the random samples must be 1 / embedding_dimension\n",
    "    emb_mat = np.random.randn(vocabulary_size, emb_dim).astype(np.float32) / np.sqrt(emb_dim) # YOUR CODE HERE\n",
    "    emb_mat = tf.Variable(emb_mat, name='Embeddings', trainable=True)\n",
    "    emb = tf.nn.embedding_lookup(emb_mat, indices)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now stack a number of layers with dimensionality given in n_hidden_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_net(units, n_hidden_list, cnn_filter_width, activation=tf.nn.relu):\n",
    "    # Use activation(units) to apply activation to units\n",
    "    for n_hidden in n_hidden_list:\n",
    "        \n",
    "        units = tf.layers.conv1d(units,\n",
    "                                 n_hidden,\n",
    "                                 cnn_filter_width,\n",
    "                                 padding='same')\n",
    "        units = activation(units)\n",
    "    return units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define your own function that returns a scalar masked cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_cross_entropy(logits, label_indices, number_of_tags, mask):\n",
    "    ground_truth_labels = tf.one_hot(label_indices, depth=number_of_tags)\n",
    "    loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(labels=ground_truth_labels, logits=logits)\n",
    "    loss_tensor *= mask\n",
    "    loss = tf.reduce_mean(loss_tensor)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put everything into a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerNetwork:\n",
    "    def __init__(self,\n",
    "                 n_tokens,\n",
    "                 n_tags,\n",
    "                 token_emb_dim=100,\n",
    "                 n_hidden_list=(128,),\n",
    "                 cnn_filter_width=7,\n",
    "                 use_batch_norm=False,\n",
    "                 embeddings_dropout=False,\n",
    "                 top_dropout=False,\n",
    "                 **kwargs):\n",
    "        # ================ Building inputs =================\n",
    "\n",
    "        self.learning_rate_ph = tf.placeholder(tf.float32, [])\n",
    "        self.dropout_keep_ph = tf.placeholder(tf.float32, [])\n",
    "        self.token_ph = tf.placeholder(tf.int32, [None, None], name='token_ind_ph')\n",
    "        self.mask_ph = tf.placeholder(tf.float32, [None, None], name='Mask_ph')\n",
    "        self.y_ph = tf.placeholder(tf.int32, [None, None], name='y_ph')\n",
    "\n",
    "        # ================== Building the network ==================\n",
    "\n",
    "        # Now embedd the indices of tokens using token_emb_dim function\n",
    "\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        emb = get_embeddings(self.token_ph, n_tokens, token_emb_dim)\n",
    "        ######################################\n",
    "\n",
    "        emb = tf.nn.dropout(emb, self.dropout_keep_ph, (tf.shape(emb)[0], 1, tf.shape(emb)[2]))\n",
    "\n",
    "        # Build a multilayer CNN on top of the embeddings.\n",
    "        # The number of units in the each layer must match\n",
    "        # corresponding number from n_hidden_list.\n",
    "        # Use ReLU activation\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        units = conv_net(emb, n_hidden_list, cnn_filter_width)\n",
    "        ######################################\n",
    "        units = tf.nn.dropout(units, self.dropout_keep_ph, (tf.shape(units)[0], 1, tf.shape(units)[2]))\n",
    "        logits = tf.layers.dense(units, n_tags, activation=None)\n",
    "        self.predictions = tf.argmax(logits, 2)\n",
    "\n",
    "        # ================= Loss and train ops =================\n",
    "        # Use cross-entropy loss. check the tf.nn.softmax_cross_entropy_with_logits_v2 function\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        self.loss = masked_cross_entropy(logits, self.y_ph, n_tags, self.mask_ph)\n",
    "        ######################################\n",
    "\n",
    "        # Create a training operation to update the network parameters.\n",
    "        # We purpose to use the Adam optimizer as it work fine for the\n",
    "        # most of the cases. Check tf.train to find an implementation.\n",
    "        # Put the train operation to the attribute self.train_op\n",
    "\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate_ph)\n",
    "        self.train_op = optimizer.minimize(self.loss)\n",
    "        ######################################\n",
    "\n",
    "        # ================= Initialize the session =================\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def __call__(self, tok_batch, mask_batch):\n",
    "        feed_dict = {self.token_ph: tok_batch,\n",
    "                     self.mask_ph: mask_batch,\n",
    "                     self.dropout_keep_ph: 1.0}\n",
    "        return self.sess.run(self.predictions, feed_dict)\n",
    "\n",
    "    def train_on_batch(self, tok_batch, tag_batch, mask_batch, dropout_keep_prob, learning_rate):\n",
    "        feed_dict = {self.token_ph: tok_batch,\n",
    "                     self.y_ph: tag_batch,\n",
    "                     self.mask_ph: mask_batch,\n",
    "                     self.dropout_keep_ph: dropout_keep_prob,\n",
    "                     self.learning_rate_ph: learning_rate}\n",
    "        self.sess.run(self.train_op, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an instance of the NerNetwork class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nernet = NerNetwork(len(token_vocab),\n",
    "                    len(tag_vocab),\n",
    "                    n_hidden_list=[100, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write the evaluation function. We need to get all predictions for the given part of the dataset and compute F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.metrics.fmeasure import precision_recall_f1\n",
    "# The function precision_recall_f1 takes two lists: y_true and y_predicted\n",
    "# the tag sequences for each sentences should be merged into one big list \n",
    "from deeppavlov.core.data.utils import zero_pad\n",
    "# zero_pad takes a batch of lists of token indices, pad it with zeros to the\n",
    "# maximal length and convert it to numpy matrix\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def eval_valid(network, batch_generator):\n",
    "    total_true = []\n",
    "    total_pred = []\n",
    "    for x, y_true in batch_generator:\n",
    "\n",
    "        # Prepare token indices from tokens batch\n",
    "        x_inds = token_vocab(x) # YOUR CODE HERE\n",
    "\n",
    "        # Pad the indices batch with zeros\n",
    "        x_batch = zero_pad(x_inds) # YOUR CODE HERE\n",
    "\n",
    "        # Get the mask using get_mask\n",
    "        mask = get_mask(x) # YOUR CODE HERE\n",
    "        \n",
    "        # We call the instance of the NerNetwork because we have defined __call__ method\n",
    "        y_inds = network(x_batch, mask)\n",
    "\n",
    "        # For every sentence in the batch extract all tags up to paddings\n",
    "        y_inds = [y_inds[n][:len(x[n])] for n, y in enumerate(y_inds)] # YOUR CODE HERE\n",
    "        y_pred = tag_vocab(y_inds)\n",
    "\n",
    "        # Add fresh predictions \n",
    "        total_true.extend(chain(*y_true))\n",
    "        total_pred.extend(chain(*y_pred))\n",
    "    res = precision_recall_f1(total_true, total_pred, print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # YOUR HYPERPARAMETER HERE\n",
    "n_epochs = 20 # YOUR HYPERPARAMETER HERE\n",
    "learning_rate = 0.001 # YOUR HYPERPARAMETER HERE\n",
    "dropout_keep_prob = 0.5 # YOUR HYPERPARAMETER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:55:09.655 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 540 phrases; correct: 521.\n",
      "\n",
      "precision:  96.48%; recall:  99.62%; FB1:  98.02\n",
      "\n",
      "\tREF: precision:  96.48%; recall:  99.62%; F1:  98.02 540\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:55:27.949 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 537 phrases; correct: 523.\n",
      "\n",
      "precision:  97.39%; recall:  100.00%; FB1:  98.68\n",
      "\n",
      "\tREF: precision:  97.39%; recall:  100.00%; F1:  98.68 537\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:55:46.326 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 535 phrases; correct: 523.\n",
      "\n",
      "precision:  97.76%; recall:  100.00%; FB1:  98.87\n",
      "\n",
      "\tREF: precision:  97.76%; recall:  100.00%; F1:  98.87 535\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:56:04.227 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 537 phrases; correct: 521.\n",
      "\n",
      "precision:  97.02%; recall:  99.62%; FB1:  98.30\n",
      "\n",
      "\tREF: precision:  97.02%; recall:  99.62%; F1:  98.30 537\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:56:22.396 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 536 phrases; correct: 522.\n",
      "\n",
      "precision:  97.39%; recall:  99.81%; FB1:  98.58\n",
      "\n",
      "\tREF: precision:  97.39%; recall:  99.81%; F1:  98.58 536\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:56:40.484 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 538 phrases; correct: 520.\n",
      "\n",
      "precision:  96.65%; recall:  99.43%; FB1:  98.02\n",
      "\n",
      "\tREF: precision:  96.65%; recall:  99.43%; F1:  98.02 538\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:56:58.746 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 538 phrases; correct: 517.\n",
      "\n",
      "precision:  96.10%; recall:  98.85%; FB1:  97.46\n",
      "\n",
      "\tREF: precision:  96.10%; recall:  98.85%; F1:  97.46 538\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:57:17.536 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 535 phrases; correct: 517.\n",
      "\n",
      "precision:  96.64%; recall:  98.85%; FB1:  97.73\n",
      "\n",
      "\tREF: precision:  96.64%; recall:  98.85%; F1:  97.73 535\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:57:37.657 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 537 phrases; correct: 521.\n",
      "\n",
      "precision:  97.02%; recall:  99.62%; FB1:  98.30\n",
      "\n",
      "\tREF: precision:  97.02%; recall:  99.62%; F1:  98.30 537\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:57:57.863 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 536 phrases; correct: 521.\n",
      "\n",
      "precision:  97.20%; recall:  99.62%; FB1:  98.39\n",
      "\n",
      "\tREF: precision:  97.20%; recall:  99.62%; F1:  98.39 536\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:58:17.9 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 534 phrases; correct: 522.\n",
      "\n",
      "precision:  97.75%; recall:  99.81%; FB1:  98.77\n",
      "\n",
      "\tREF: precision:  97.75%; recall:  99.81%; F1:  98.77 534\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:58:36.562 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 534 phrases; correct: 518.\n",
      "\n",
      "precision:  97.00%; recall:  99.04%; FB1:  98.01\n",
      "\n",
      "\tREF: precision:  97.00%; recall:  99.04%; F1:  98.01 534\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:58:54.793 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 535 phrases; correct: 522.\n",
      "\n",
      "precision:  97.57%; recall:  99.81%; FB1:  98.68\n",
      "\n",
      "\tREF: precision:  97.57%; recall:  99.81%; F1:  98.68 535\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:59:12.746 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 536 phrases; correct: 520.\n",
      "\n",
      "precision:  97.01%; recall:  99.43%; FB1:  98.21\n",
      "\n",
      "\tREF: precision:  97.01%; recall:  99.43%; F1:  98.21 536\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:59:31.235 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 535 phrases; correct: 518.\n",
      "\n",
      "precision:  96.82%; recall:  99.04%; FB1:  97.92\n",
      "\n",
      "\tREF: precision:  96.82%; recall:  99.04%; F1:  97.92 535\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:59:49.526 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 537 phrases; correct: 522.\n",
      "\n",
      "precision:  97.21%; recall:  99.81%; FB1:  98.49\n",
      "\n",
      "\tREF: precision:  97.21%; recall:  99.81%; F1:  98.49 537\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-19 00:00:07.696 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 536 phrases; correct: 523.\n",
      "\n",
      "precision:  97.57%; recall:  100.00%; FB1:  98.77\n",
      "\n",
      "\tREF: precision:  97.57%; recall:  100.00%; F1:  98.77 536\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-19 00:00:25.856 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 535 phrases; correct: 520.\n",
      "\n",
      "precision:  97.20%; recall:  99.43%; FB1:  98.30\n",
      "\n",
      "\tREF: precision:  97.20%; recall:  99.43%; F1:  98.30 535\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-19 00:00:44.5 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 533 phrases; correct: 516.\n",
      "\n",
      "precision:  96.81%; recall:  98.66%; FB1:  97.73\n",
      "\n",
      "\tREF: precision:  96.81%; recall:  98.66%; F1:  97.73 533\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-19 00:01:05.788 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 19512 tokens with 523 phrases; found: 536 phrases; correct: 520.\n",
      "\n",
      "precision:  97.01%; recall:  99.43%; FB1:  98.21\n",
      "\n",
      "\tREF: precision:  97.01%; recall:  99.43%; F1:  98.21 536\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for x, y in data_iterator.gen_batches(batch_size, 'train'):\n",
    "        # Convert tokens to indices via Vocab\n",
    "        x_inds = token_vocab(x) # YOUR CODE \n",
    "        # Convert tags to indices via Vocab\n",
    "        y_inds = tag_vocab(y) # YOUR CODE \n",
    "        \n",
    "        # Pad every sample with zeros to the maximal length\n",
    "        x_batch = zero_pad(x_inds)\n",
    "        y_batch = zero_pad(y_inds)\n",
    "\n",
    "        mask = get_mask(x)\n",
    "        nernet.train_on_batch(x_batch, y_batch, mask, dropout_keep_prob, learning_rate)\n",
    "    print('Evaluating the model on valid part of the dataset')\n",
    "    eval_valid(nernet, data_iterator.gen_batches(batch_size, 'valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to infer the model on our sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "о : O\n",
      "разъяснении : O\n",
      "Определения : O\n",
      "Конституционного : O\n",
      "Суда : O\n",
      "Российской : O\n",
      "Федерации : O\n",
      "от : B-REF\n",
      "24 : I-REF\n",
      "марта : I-REF\n",
      "2015 : I-REF\n",
      "года : I-REF\n",
      "№ : I-REF\n",
      "720-О : I-REF\n",
      "город : O\n",
      "Санкт-Петербург : O\n"
     ]
    }
   ],
   "source": [
    "sentence = 'о разъяснении Определения Конституционного Суда Российской Федерации от 24 марта 2015 года № 720-О город Санкт-Петербург'\n",
    "x = [sentence.split()]\n",
    "\n",
    "x_inds = token_vocab(x)\n",
    "x_batch = zero_pad(x_inds)\n",
    "mask = get_mask(x)\n",
    "y_inds = nernet(x_batch, mask)\n",
    "for token,tag in zip(x[0],tag_vocab(y_inds)[0]):\n",
    "    print(token + \" : \" + tag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
