{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the neural network from save_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\stron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     C:\\Users\\stron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     C:\\Users\\stron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "2018-11-30 22:08:54.762 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 48: No load path is set for SimpleVocabulary in 'infer' mode. Using save path instead\n",
      "2018-11-30 22:08:54.784 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 48: No load path is set for SimpleVocabulary in 'infer' mode. Using save path instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save_module\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-30 22:09:02.753 INFO in 'tensorflow'['tf_logging'] at line 115: Restoring parameters from save_module\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
    "from deeppavlov.dataset_readers.conll2003_reader import Conll2003DatasetReader\n",
    "from deeppavlov.metrics.fmeasure import precision_recall_f1\n",
    "# The function precision_recall_f1 takes two lists: y_true and y_predicted\n",
    "# the tag sequences for each sentences should be merged into one big list\n",
    "from deeppavlov.core.data.utils import zero_pad\n",
    "# zero_pad takes a batch of lists of token indices, pad it with zeros to the\n",
    "# maximal length and convert it to numpy matrix\n",
    "from itertools import chain\n",
    "from deeppavlov.core.data.data_learning_iterator import DataLearningIterator\n",
    "from deeppavlov.models.preprocessors.mask import Mask\n",
    "\n",
    "def get_embeddings(indices, vocabulary_size, emb_dim):\n",
    "    # Initialize the random gaussian matrix with dimensions [vocabulary_size, embedding_dimension]\n",
    "    # The **VARIANCE** of the random samples must be 1 / embedding_dimension\n",
    "    emb_mat = np.random.randn(vocabulary_size, emb_dim).astype(np.float32) / np.sqrt(emb_dim)\n",
    "    # YOUR CODE HERE\n",
    "    emb_mat = tf.Variable(emb_mat, name='Embeddings', trainable=True)\n",
    "    emb = tf.nn.embedding_lookup(emb_mat, indices)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def conv_net(units, n_hidden_list, cnn_filter_width, activation=tf.nn.relu):\n",
    "    # Use activation(units) to apply activation to units\n",
    "    for n_hidden in n_hidden_list:\n",
    "        units = tf.layers.conv1d(units,\n",
    "                                 n_hidden,\n",
    "                                 cnn_filter_width,\n",
    "                                 padding='same')\n",
    "        units = activation(units)\n",
    "    return units\n",
    "\n",
    "\n",
    "def masked_cross_entropy(logits, label_indices, number_of_tags, mask):\n",
    "    ground_truth_labels = tf.one_hot(label_indices, depth=number_of_tags)\n",
    "    loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(labels=ground_truth_labels, logits=logits)\n",
    "    loss_tensor *= mask\n",
    "    loss = tf.reduce_mean(loss_tensor)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def eval_valid(network, batch_generator):\n",
    "    total_true = []\n",
    "    total_pred = []\n",
    "    for x, y_true in batch_generator:\n",
    "        # Prepare token indices from tokens batch\n",
    "        x_inds = token_vocab(x)  # YOUR CODE HERE\n",
    "\n",
    "        # Pad the indices batch with zeros\n",
    "        x_batch = zero_pad(x_inds)  # YOUR CODE HERE\n",
    "\n",
    "        # Get the mask using get_mask\n",
    "        mask = get_mask(x)  # YOUR CODE HERE\n",
    "\n",
    "        # We call the instance of the NerNetwork because we have defined __call__ method\n",
    "        y_inds = network(x_batch, mask)\n",
    "\n",
    "        # For every sentence in the batch extract all tags up to paddings\n",
    "        y_inds = [y_inds[n][:len(x[n])] for n, y in enumerate(y_inds)]  # YOUR CODE HERE\n",
    "        y_pred = tag_vocab(y_inds)\n",
    "\n",
    "        # Add fresh predictions\n",
    "        total_true.extend(chain(*y_true))\n",
    "        total_pred.extend(chain(*y_pred))\n",
    "    res = precision_recall_f1(total_true, total_pred, print_results=True)\n",
    "\n",
    "\n",
    "class NerNetwork:\n",
    "    def __init__(self,\n",
    "                 n_tokens,\n",
    "                 n_tags,\n",
    "                 token_emb_dim=100,\n",
    "                 n_hidden_list=(128,),\n",
    "                 cnn_filter_width=7,\n",
    "                 use_batch_norm=False,\n",
    "                 embeddings_dropout=False,\n",
    "                 top_dropout=False,\n",
    "                 **kwargs):\n",
    "        # ================ Building inputs =================\n",
    "\n",
    "        self.learning_rate_ph = tf.placeholder(tf.float32, [])\n",
    "        self.dropout_keep_ph = tf.placeholder(tf.float32, [])\n",
    "        self.token_ph = tf.placeholder(tf.int32, [None, None], name='token_ind_ph')\n",
    "        self.mask_ph = tf.placeholder(tf.float32, [None, None], name='Mask_ph')\n",
    "        self.y_ph = tf.placeholder(tf.int32, [None, None], name='y_ph')\n",
    "\n",
    "        # ================== Building the network ==================\n",
    "\n",
    "        # Now embedd the indices of tokens using token_emb_dim function\n",
    "\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        emb = get_embeddings(self.token_ph, n_tokens, token_emb_dim)\n",
    "        ######################################\n",
    "\n",
    "        emb = tf.nn.dropout(emb, self.dropout_keep_ph, (tf.shape(emb)[0], 1, tf.shape(emb)[2]))\n",
    "\n",
    "        # Build a multilayer CNN on top of the embeddings.\n",
    "        # The number of units in the each layer must match\n",
    "        # corresponding number from n_hidden_list.\n",
    "        # Use ReLU activation\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        units = conv_net(emb, n_hidden_list, cnn_filter_width)\n",
    "        ######################################\n",
    "        units = tf.nn.dropout(units, self.dropout_keep_ph, (tf.shape(units)[0], 1, tf.shape(units)[2]))\n",
    "        logits = tf.layers.dense(units, n_tags, activation=None)\n",
    "        self.predictions = tf.argmax(logits, 2)\n",
    "\n",
    "        # ================= Loss and train ops =================\n",
    "        # Use cross-entropy loss. check the tf.nn.softmax_cross_entropy_with_logits_v2 function\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        self.loss = masked_cross_entropy(logits, self.y_ph, n_tags, self.mask_ph)\n",
    "        ######################################\n",
    "\n",
    "        # Create a training operation to update the network parameters.\n",
    "        # We purpose to use the Adam optimizer as it work fine for the\n",
    "        # most of the cases. Check tf.train to find an implementation.\n",
    "        # Put the train operation to the attribute self.train_op\n",
    "\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate_ph)\n",
    "        self.train_op = optimizer.minimize(self.loss)\n",
    "        ######################################\n",
    "\n",
    "        # ================= Initialize the session =================\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "    def __call__(self, tok_batch, mask_batch):\n",
    "        feed_dict = {self.token_ph: tok_batch,\n",
    "                     self.mask_ph: mask_batch,\n",
    "                     self.dropout_keep_ph: 1.0}\n",
    "        return self.sess.run(self.predictions, feed_dict)\n",
    "\n",
    "    def train_on_batch(self, tok_batch, tag_batch, mask_batch, dropout_keep_prob, learning_rate):\n",
    "        feed_dict = {self.token_ph: tok_batch,\n",
    "                     self.y_ph: tag_batch,\n",
    "                     self.mask_ph: mask_batch,\n",
    "                     self.dropout_keep_ph: dropout_keep_prob,\n",
    "                     self.learning_rate_ph: learning_rate}\n",
    "        self.sess.run(self.train_op, feed_dict)\n",
    "        # Now, save the graph\n",
    "\n",
    "dataset = Conll2003DatasetReader().read('data')\n",
    "get_mask = Mask()\n",
    "data_iterator = DataLearningIterator(dataset)\n",
    "\n",
    "special_tokens = ['<UNK>']\n",
    "token_vocab = SimpleVocabulary(special_tokens, save_path='model/token.dict')\n",
    "tag_vocab = SimpleVocabulary(save_path='model/tag.dict')\n",
    "\n",
    "all_tokens_by_sentences = [tokens for tokens, tags in dataset['train']]\n",
    "all_tags_by_sentences = [tags for tokens, tags in dataset['train']]\n",
    "\n",
    "token_vocab.fit(all_tokens_by_sentences)\n",
    "tag_vocab.fit(all_tags_by_sentences)\n",
    "\n",
    "nernet = NerNetwork(len(token_vocab),\n",
    "                            len(tag_vocab),\n",
    "                            n_hidden_list=[100, 100])\n",
    "nernet.saver.restore(nernet.sess,'save_module')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the  loss of  neural  network on 1648 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-30 22:46:05.812 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 286: processed 290143 tokens with 5136 phrases; found: 5224 phrases; correct: 4928.\n",
      "\n",
      "precision:  94.33%; recall:  95.95%; FB1:  95.14\n",
      "\n",
      "\tREF: precision:  94.33%; recall:  95.95%; F1:  95.14 5224\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_valid(nernet, data_iterator.gen_batches(16483//10, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadData started\n",
      "The average time per one document :  0.03149499320983887  seconds.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "from file_parser import loadData,repl\n",
    "\n",
    "\n",
    "def estimateTime(numDocuments = 1000):\n",
    "    data = loadData()[0:numDocuments]\n",
    "    wholeTime = 0\n",
    "    for k in range(numDocuments):\n",
    "        with open('Decision_files/' + ('_').join(data[k][\"doc_id_from\"].split('/')) + '.txt',\n",
    "                      encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                start = time.time()\n",
    "                unsplittedLine = re.sub(r'[^a-яА-ЯёЁ0-9]', repl, line)\n",
    "                x = [unsplittedLine.split()]\n",
    "                x_inds = token_vocab(x)\n",
    "                x_batch = zero_pad(x_inds)\n",
    "                mask = get_mask(x)\n",
    "                y_inds = nernet(x_batch, mask)\n",
    "                end = time.time()\n",
    "                wholeTime = wholeTime + (end - start)\n",
    "    wholeTime = wholeTime / numDocuments\n",
    "    print(\"The average time per one document : \" ,wholeTime, \" seconds.\")\n",
    "    \n",
    "estimateTime()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
